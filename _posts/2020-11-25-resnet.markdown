---
layout: post
title:  "ResNet architecture. Residual blocks."
tags: [cv, feature extractor]
excerpt: It's pilot note. Simple and not bad image feature extractor network with low count of parameters that realise powerful idea of skip connections.
style: note
---
# Main idea

Main idea of ResNet is use residual blocks or "building" blocks of this network. For example exist "ideal"
feature map __out_feature_map__, instead approach to __out_feature_map__ as this make simple CNN(like VGG):

```
cnn_out_feature_map = CNN_block_fisrt(inp_feature_map)
```

ResNet get __out_feature_map__ in the following way:

```
out_feature_map = RES_block_first(inp_feature_map) + inp_feature_map
```

Where the output of RES_block_first is residual, thus we approach to __out_feature_map__ using residuals of res. blocks.
Each residual block uses a skip connection or shortcut. *My intuitive* interpretation: association with
gradient boosting on trees, alike each tree in GBT generate tails, res. blocks generate residuals. 
Also, shortcuts solve the problem of vanishing gradients(during backprop, gradient come through shortcut, bypassing many conv layers which may
decrease gradient).

# Residual block or bottleneck

On figure A a main building block of this net, bottleneck, there are 2 types for small and more largest ResNet's. Shortcut performing a 
identity mapping function or f(x) = x, in implementations used CONV+BN+ReLU.

![resnet-a](/images/resnet/resnet-a.png)

> Figure A.

Example of Resnet34 architecture on image B, various colors are marked blocks with different size of feature map. ResNet
use 2 types of shortcut:
* Shortcut 1, just perform identity mapping function, between blocks with same feature map's size.
* Shortcut 2, using when need change feature map size, can change it in 2 different ways:
  * Non-parametrical, using pooling 1x1/2, but it not change num. of channels, for this add(in 18, 34 architectures) zero feature maps or delete(other architectures) feature maps;
  * Parametreical, it best[1], using conv 1x1/2 it increase(18/34) or decrease(other architectures) number of channels

![resnet-b](/images/resnet/resnet-b.png)

> Figure B.

# Resnet architecture

On figure B(top) a plot resnet architectures, some convolution layers have 'same' padding(HxW of input = HxW of output),
size of input feature maps compute based on size of input image - 224x224. Bottom table show number of parameters
for architecture with different count of layers.

![resnet-c](/images/resnet/resnet-c.png)

> Figure C.

Links

***

1. [Nice base explanation;](https://towardsdatascience.com/intuition-behind-residual-neural-networks-fa5d2996b2c7)
2. [Another one;](https://neurohive.io/ru/vidy-nejrosetej/resnet-34-50-101)
3. [Implementation by keras team;](https://github.com/keras-team/keras-applications/blob/master/keras_applications/resnet50.py)
4. [Original paper;](https://arxiv.org/pdf/1512.03385.pdf)
5. [Apply ReLU only for residuals..?.](https://arxiv.org/pdf/1603.05027.pdf)


